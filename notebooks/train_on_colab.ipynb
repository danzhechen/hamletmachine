{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Hamletmachine LLM Training on Google Colab Pro\n",
    "\n",
    "This notebook trains the hamletmachine language model using Google Colab's GPU.\n",
    "\n",
    "## üéì Colab Pro Benefits (Free for Students)\n",
    "\n",
    "- **Better GPUs**: T4, P100, or V100 (vs T4 only on free tier)\n",
    "- **Longer Sessions**: Up to 24 hours (vs ~9-12 hours on free tier)\n",
    "- **More RAM**: Better for larger models and datasets\n",
    "- **Better Availability**: Priority GPU access\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí **GPU**\n",
    "2. **Mount Google Drive** (recommended, for saving checkpoints): Run the mount cell below\n",
    "3. **Clone from GitHub**: Run the clone cell to get the latest code\n",
    "4. **Run all cells** in order\n",
    "\n",
    "## Notes\n",
    "- Checkpoints are saved to Google Drive (if mounted) for persistence\n",
    "- Training progress is logged to TensorBoard\n",
    "- Settings are auto-optimized based on your GPU type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## 1. Setup Environment & GPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and detect GPU type\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"üîç Detecting GPU...\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.2f} GB\")\n",
    "    \n",
    "    # Detect GPU type and set optimal defaults\n",
    "    gpu_type = None\n",
    "    if 'T4' in gpu_name:\n",
    "        gpu_type = 'T4'\n",
    "        print(\"   Type: T4 (16GB) - Good for GPT-2 small/medium\")\n",
    "    elif 'P100' in gpu_name:\n",
    "        gpu_type = 'P100'\n",
    "        print(\"   Type: P100 (16GB) - Excellent! Can handle GPT-2 medium/large\")\n",
    "    elif 'V100' in gpu_name:\n",
    "        gpu_type = 'V100'\n",
    "        print(\"   Type: V100 (16GB/32GB) - Excellent! Can handle larger models\")\n",
    "    elif 'A100' in gpu_name:\n",
    "        gpu_type = 'A100'\n",
    "        print(\"   Type: A100 - Premium! Can handle very large models\")\n",
    "    else:\n",
    "        gpu_type = 'UNKNOWN'\n",
    "        print(f\"   Type: Unknown GPU - Using conservative settings\")\n",
    "    \n",
    "    # Store GPU info for later use\n",
    "    GPU_TYPE = gpu_type\n",
    "    GPU_MEMORY_GB = gpu_memory\n",
    "    \n",
    "    print(f\"\\nüí° Recommended settings for {gpu_type}:\")\n",
    "    if gpu_type == 'T4':\n",
    "        print(\"   - Model: GPT-2 small (124M) or medium (350M)\")\n",
    "        print(\"   - Batch size: 4-8 per device\")\n",
    "        print(\"   - FP16: Enabled (recommended)\")\n",
    "    elif gpu_type in ['P100', 'V100']:\n",
    "        print(\"   - Model: GPT-2 medium (350M) or large (774M)\")\n",
    "        print(\"   - Batch size: 8-16 per device\")\n",
    "        print(\"   - FP16: Enabled (recommended)\")\n",
    "    elif gpu_type == 'A100':\n",
    "        print(\"   - Model: GPT-2 large (774M) or XL (1.5B)\")\n",
    "        print(\"   - Batch size: 16-32 per device\")\n",
    "        print(\"   - FP16: Enabled (recommended)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected! Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    GPU_TYPE = None\n",
    "    GPU_MEMORY_GB = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive (recommended - for saving checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set checkpoint directory\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/hamletmachine/checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"‚úÖ Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "print(\"üí° Checkpoints saved to Drive will persist after session ends!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone from GitHub\n",
    "!git clone https://github.com/danzhechen/hamletmachine.git\n",
    "%cd hamletmachine\n",
    "\n",
    "print(\"‚úÖ Repository cloned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-header"
   },
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install project dependencies\n",
    "!pip install -q transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0 tokenizers>=0.15.0\n",
    "!pip install -q torch>=2.1.0 numpy>=1.24.0 pandas>=2.0.0 pyyaml>=6.0\n",
    "!pip install -q tensorboard>=2.15.0 tqdm>=4.66.0\n",
    "\n",
    "# Optional: Install WandB for experiment tracking\n",
    "# !pip install -q wandb\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-project"
   },
   "outputs": [],
   "source": [
    "# Add project to Python path\n",
    "import sys\n",
    "sys.path.insert(0, '/content/hamletmachine')\n",
    "\n",
    "print(\"‚úÖ Project added to Python path!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload-data-header"
   },
   "source": [
    "## 3. Upload or Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-data"
   },
   "outputs": [],
   "source": [
    "# Option 1: Upload processed dataset files\n",
    "# Use the file browser on the left to upload:\n",
    "# - data/processed/train.jsonl\n",
    "# - data/processed/validation.jsonl\n",
    "# - data/processed/test.jsonl\n",
    "\n",
    "# Option 2: Process data on Colab (if you uploaded raw training materials)\n",
    "# from hamletmachine.data.pipeline import DataPipeline\n",
    "# pipeline = DataPipeline(config_path='configs/data_config.yaml')\n",
    "# pipeline.run()\n",
    "\n",
    "# Verify data files exist\n",
    "data_dir = '/content/hamletmachine/data/processed'\n",
    "if os.path.exists(data_dir):\n",
    "    files = os.listdir(data_dir)\n",
    "    jsonl_files = [f for f in files if f.endswith('.jsonl')]\n",
    "    if jsonl_files:\n",
    "        print(f\"‚úÖ Data files found: {jsonl_files}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  No JSONL files found in {data_dir}\")\n",
    "        print(\"Please upload your processed dataset files.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Data directory not found: {data_dir}\")\n",
    "    print(\"Please create the directory and upload your processed dataset files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config-header"
   },
   "source": [
    "## 4. Configure Training (Auto-Optimized for Your GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-config"
   },
   "outputs": [],
   "source": [
    "# Auto-configure training based on GPU type\n",
    "import yaml\n",
    "\n",
    "# GPU-specific optimizations\n",
    "def get_gpu_optimized_config(gpu_type, gpu_memory_gb):\n",
    "    \"\"\"Get optimized config based on GPU type.\"\"\"\n",
    "    \n",
    "    if gpu_type == 'T4':\n",
    "        # T4: Good for small/medium models\n",
    "        return {\n",
    "            'model_architecture': 'gpt2',  # 124M parameters\n",
    "            'batch_size': 8,  # Can go up to 8 on T4 with FP16\n",
    "            'gradient_accumulation': 4,  # Effective batch size: 32\n",
    "            'fp16': True,\n",
    "            'max_seq_length': 1024,\n",
    "        }\n",
    "    elif gpu_type == 'P100':\n",
    "        # P100: Excellent for medium/large models\n",
    "        return {\n",
    "            'model_architecture': 'gpt2-medium',  # 350M parameters\n",
    "            'batch_size': 12,  # P100 can handle larger batches\n",
    "            'gradient_accumulation': 4,  # Effective batch size: 48\n",
    "            'fp16': True,\n",
    "            'max_seq_length': 1024,\n",
    "        }\n",
    "    elif gpu_type == 'V100':\n",
    "        # V100: Excellent for large models\n",
    "        return {\n",
    "            'model_architecture': 'gpt2-medium',  # Can try gpt2-large (774M)\n",
    "            'batch_size': 16,  # V100 can handle even larger batches\n",
    "            'gradient_accumulation': 4,  # Effective batch size: 64\n",
    "            'fp16': True,\n",
    "            'max_seq_length': 1024,\n",
    "        }\n",
    "    elif gpu_type == 'A100':\n",
    "        # A100: Premium, can handle very large models\n",
    "        return {\n",
    "            'model_architecture': 'gpt2-large',  # 774M parameters\n",
    "            'batch_size': 24,  # A100 can handle very large batches\n",
    "            'gradient_accumulation': 4,  # Effective batch size: 96\n",
    "            'fp16': True,\n",
    "            'max_seq_length': 1024,\n",
    "        }\n",
    "    else:\n",
    "        # Conservative defaults for unknown GPUs\n",
    "        return {\n",
    "            'model_architecture': 'gpt2',  # Start small\n",
    "            'batch_size': 4,\n",
    "            'gradient_accumulation': 4,\n",
    "            'fp16': True,\n",
    "            'max_seq_length': 1024,\n",
    "        }\n",
    "\n",
    "# Get optimized config\n",
    "if 'GPU_TYPE' in globals() and GPU_TYPE:\n",
    "    gpu_config = get_gpu_optimized_config(GPU_TYPE, GPU_MEMORY_GB)\n",
    "    print(f\"üéØ Auto-configured for {GPU_TYPE} GPU:\")\n",
    "    print(f\"   Model: {gpu_config['model_architecture']}\")\n",
    "    print(f\"   Batch size: {gpu_config['batch_size']} per device\")\n",
    "    print(f\"   Gradient accumulation: {gpu_config['gradient_accumulation']}\")\n",
    "    print(f\"   Effective batch size: {gpu_config['batch_size'] * gpu_config['gradient_accumulation']}\")\n",
    "    print(f\"   FP16: {gpu_config['fp16']}\")\n",
    "else:\n",
    "    # Fallback if GPU not detected\n",
    "    gpu_config = get_gpu_optimized_config('T4', 16)\n",
    "    print(\"‚ö†Ô∏è  Using default T4 settings (GPU not detected)\")\n",
    "\n",
    "# Create full training config\n",
    "config_path = '/content/hamletmachine/configs/train_config.yaml'\n",
    "if not os.path.exists(config_path):\n",
    "    config = {\n",
    "        'model': {\n",
    "            'architecture': gpu_config['model_architecture'],\n",
    "        },\n",
    "        'training': {\n",
    "            'output_dir': CHECKPOINT_DIR if 'CHECKPOINT_DIR' in globals() else '/content/models/checkpoints',\n",
    "            'num_train_epochs': 3,\n",
    "            'per_device_train_batch_size': gpu_config['batch_size'],\n",
    "            'per_device_eval_batch_size': gpu_config['batch_size'],\n",
    "            'gradient_accumulation_steps': gpu_config['gradient_accumulation'],\n",
    "            'learning_rate': 5.0e-5,\n",
    "            'warmup_steps': 100,\n",
    "            'logging_steps': 10,\n",
    "            'save_steps': 500,\n",
    "            'eval_steps': 500,\n",
    "            'save_total_limit': 3,\n",
    "            'fp16': gpu_config['fp16'],\n",
    "            'dataloader_pin_memory': True,\n",
    "        },\n",
    "        'data': {\n",
    "            'train_file': '/content/hamletmachine/data/processed/train.jsonl',\n",
    "            'validation_file': '/content/hamletmachine/data/processed/validation.jsonl',\n",
    "            'max_seq_length': gpu_config['max_seq_length'],\n",
    "        },\n",
    "        'tokenizer': {\n",
    "            'tokenizer_name': 'gpt2',\n",
    "        },\n",
    "        'logging': {\n",
    "            'logger': 'tensorboard',\n",
    "            'logging_dir': '/content/logs',\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save config\n",
    "    os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "    print(f\"\\n‚úÖ Created optimized config at {config_path}\")\n",
    "else:\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(f\"\\n‚úÖ Loaded existing config from {config_path}\")\n",
    "    # Override with GPU-optimized settings if not already set\n",
    "    if 'GPU_TYPE' in globals() and GPU_TYPE:\n",
    "        config['model']['architecture'] = gpu_config['model_architecture']\n",
    "        config['training']['per_device_train_batch_size'] = gpu_config['batch_size']\n",
    "        config['training']['per_device_eval_batch_size'] = gpu_config['batch_size']\n",
    "        config['training']['gradient_accumulation_steps'] = gpu_config['gradient_accumulation']\n",
    "        config['training']['fp16'] = gpu_config['fp16']\n",
    "        print(\"   (Updated with GPU-optimized settings)\")\n",
    "\n",
    "print(\"\\nüìã Training Configuration:\")\n",
    "print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train-header"
   },
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-model"
   },
   "outputs": [],
   "source": [
    "# Import training modules\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "print(\"‚úÖ Training modules imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-tokenizer"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer_name = config['tokenizer']['tokenizer_name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded: {tokenizer_name}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": [
    "# Load model with appropriate dtype\n",
    "model_name = config['model']['architecture']\n",
    "use_fp16 = config['training']['fp16']\n",
    "\n",
    "print(f\"üì• Loading model: {model_name}\")\n",
    "print(f\"   Using FP16: {use_fp16}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if use_fp16 else torch.float32\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Calculate model size\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model_name}\")\n",
    "print(f\"   Parameters: {num_params / 1e6:.2f}M\")\n",
    "print(f\"   Trainable: {num_trainable / 1e6:.2f}M\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    memory_reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"   GPU Memory - Allocated: {memory_allocated:.2f} GB, Reserved: {memory_reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-dataset"
   },
   "outputs": [],
   "source": [
    "# Load and tokenize datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=config['data']['max_seq_length'],\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "print(\"üì• Loading datasets...\")\n",
    "\n",
    "# Load JSONL files\n",
    "train_dataset = load_dataset('json', data_files=config['data']['train_file'], split='train')\n",
    "val_dataset = load_dataset('json', data_files=config['data']['validation_file'], split='train')\n",
    "\n",
    "print(f\"   Training examples: {len(train_dataset)}\")\n",
    "print(f\"   Validation examples: {len(val_dataset)}\")\n",
    "\n",
    "# Tokenize\n",
    "print(\"üî§ Tokenizing datasets...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing training set\"\n",
    ")\n",
    "val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation set\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Datasets loaded and tokenized!\")\n",
    "print(f\"   Training examples: {len(train_dataset)}\")\n",
    "print(f\"   Validation examples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-trainer"
   },
   "outputs": [],
   "source": [
    "# Setup data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config['training']['output_dir'],\n",
    "    num_train_epochs=config['training']['num_train_epochs'],\n",
    "    per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "    learning_rate=config['training']['learning_rate'],\n",
    "    warmup_steps=config['training']['warmup_steps'],\n",
    "    logging_steps=config['training']['logging_steps'],\n",
    "    save_steps=config['training']['save_steps'],\n",
    "    eval_steps=config['training']['eval_steps'],\n",
    "    save_total_limit=config['training']['save_total_limit'],\n",
    "    fp16=config['training']['fp16'],\n",
    "    dataloader_pin_memory=config['training'].get('dataloader_pin_memory', True),\n",
    "    logging_dir=config['logging']['logging_dir'],\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    report_to='tensorboard' if config['logging']['logger'] == 'tensorboard' else None,\n",
    "    # Gradient clipping for stability\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer setup complete!\")\n",
    "print(f\"   Effective batch size: {config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"   Total training steps: {len(train_dataset) // (config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']) * config['training']['num_train_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start-training"
   },
   "outputs": [],
   "source": [
    "# Start training!\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"   Model: {config['model']['architecture']}\")\n",
    "print(f\"   GPU: {GPU_TYPE if 'GPU_TYPE' in globals() else 'Unknown'}\")\n",
    "print(f\"   Checkpoints: {config['training']['output_dir']}\")\n",
    "print(f\"   TensorBoard: {config['logging']['logging_dir']}\")\n",
    "print(f\"   Epochs: {config['training']['num_train_epochs']}\")\n",
    "print(\"\\nüí° With Colab Pro, you have up to 24 hours - training should complete comfortably!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-model"
   },
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_dir = os.path.join(config['training']['output_dir'], 'final_model')\n",
    "trainer.save_model(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "print(f\"‚úÖ Final model saved to: {final_model_dir}\")\n",
    "print(f\"üí° Model is saved to Google Drive and will persist after session ends!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tensorboard-header"
   },
   "source": [
    "## 6. Monitor Training (TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tensorboard"
   },
   "outputs": [],
   "source": [
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard\n",
    "%tensorboard --logdir {config['logging']['logging_dir']} --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-header"
   },
   "source": [
    "## 7. Download Checkpoints (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-checkpoints"
   },
   "outputs": [],
   "source": [
    "# If checkpoints are saved to Colab storage (not Drive), download them\n",
    "# This creates a zip file you can download\n",
    "\n",
    "import shutil\n",
    "\n",
    "checkpoint_dir = config['training']['output_dir']\n",
    "if os.path.exists(checkpoint_dir) and not checkpoint_dir.startswith('/content/drive'):\n",
    "    zip_path = '/content/hamletmachine_checkpoints.zip'\n",
    "    shutil.make_archive(\n",
    "        zip_path.replace('.zip', ''),\n",
    "        'zip',\n",
    "        checkpoint_dir\n",
    "    )\n",
    "    print(f\"‚úÖ Checkpoints zipped: {zip_path}\")\n",
    "    print(\"Download from: Files ‚Üí hamletmachine_checkpoints.zip\")\n",
    "else:\n",
    "    print(\"‚úÖ Checkpoints are saved to Google Drive - no download needed!\")\n",
    "    print(f\"   Access them at: {checkpoint_dir}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
