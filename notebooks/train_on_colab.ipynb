{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Hamletmachine LLM Training on Google Colab Pro\n",
        "\n",
        "This notebook trains the hamletmachine language model using Google Colab's GPU.\n",
        "\n",
        "## üéì Colab Pro Benefits (Free for Students)\n",
        "\n",
        "- **Better GPUs**: T4, P100, or V100 (vs T4 only on free tier)\n",
        "- **Longer Sessions**: Up to 24 hours (vs ~9-12 hours on free tier)\n",
        "- **More RAM**: Better for larger models and datasets\n",
        "- **Better Availability**: Priority GPU access\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí **GPU**\n",
        "2. **Mount Google Drive** (recommended, for saving checkpoints): Run the mount cell below\n",
        "3. **Clone from GitHub**: Run the clone cell to get the latest code\n",
        "4. **Run all cells** in order\n",
        "\n",
        "## Notes\n",
        "- Checkpoints are saved to Google Drive (if mounted) for persistence\n",
        "- Training progress is logged to TensorBoard\n",
        "- Settings are auto-optimized based on your GPU type\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "## 1. Setup Environment & GPU Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check-gpu"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability and detect GPU type\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"üîç Detecting GPU...\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    \n",
        "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
        "    print(f\"   Memory: {gpu_memory:.2f} GB\")\n",
        "    \n",
        "    # Detect GPU type and set optimal defaults\n",
        "    gpu_type = None\n",
        "    if 'T4' in gpu_name:\n",
        "        gpu_type = 'T4'\n",
        "        print(\"   Type: T4 (16GB) - Good for GPT-2 small/medium\")\n",
        "    elif 'P100' in gpu_name:\n",
        "        gpu_type = 'P100'\n",
        "        print(\"   Type: P100 (16GB) - Excellent! Can handle GPT-2 medium/large\")\n",
        "    elif 'V100' in gpu_name:\n",
        "        gpu_type = 'V100'\n",
        "        print(\"   Type: V100 (16GB/32GB) - Excellent! Can handle larger models\")\n",
        "    elif 'A100' in gpu_name:\n",
        "        gpu_type = 'A100'\n",
        "        print(\"   Type: A100 - Premium! Can handle very large models\")\n",
        "    else:\n",
        "        gpu_type = 'UNKNOWN'\n",
        "        print(f\"   Type: Unknown GPU - Using conservative settings\")\n",
        "    \n",
        "    # Store GPU info for later use\n",
        "    GPU_TYPE = gpu_type\n",
        "    GPU_MEMORY_GB = gpu_memory\n",
        "    \n",
        "    print(f\"\\nüí° Recommended settings for {gpu_type}:\")\n",
        "    if gpu_type == 'T4':\n",
        "        print(\"   - Model: GPT-2 small (124M) or medium (350M)\")\n",
        "        print(\"   - Batch size: 4-8 per device\")\n",
        "        print(\"   - FP16: Enabled (recommended)\")\n",
        "    elif gpu_type in ['P100', 'V100']:\n",
        "        print(\"   - Model: GPT-2 medium (350M) or large (774M)\")\n",
        "        print(\"   - Batch size: 8-16 per device\")\n",
        "        print(\"   - FP16: Enabled (recommended)\")\n",
        "    elif gpu_type == 'A100':\n",
        "        print(\"   - Model: GPT-2 large (774M) or XL (1.5B)\")\n",
        "        print(\"   - Batch size: 16-32 per device\")\n",
        "        print(\"   - FP16: Enabled (recommended)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected! Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "    GPU_TYPE = None\n",
        "    GPU_MEMORY_GB = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (recommended - for saving checkpoints)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set checkpoint directory\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/hamletmachine/checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "print(f\"‚úÖ Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
        "print(\"üí° Checkpoints saved to Drive will persist after session ends!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone-repo"
      },
      "outputs": [],
      "source": [
        "# Clone from GitHub\n",
        "!git clone https://github.com/danzhechen/hamletmachine.git\n",
        "%cd hamletmachine\n",
        "\n",
        "print(\"‚úÖ Repository cloned!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-header"
      },
      "source": [
        "## 2. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Install project dependencies\n",
        "!pip install -q transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0 tokenizers>=0.15.0\n",
        "!pip install -q torch>=2.1.0 numpy>=1.24.0 pandas>=2.0.0 pyyaml>=6.0\n",
        "!pip install -q tensorboard>=2.15.0 tqdm>=4.66.0\n",
        "\n",
        "# Optional: Install WandB for experiment tracking\n",
        "# !pip install -q wandb\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-project"
      },
      "outputs": [],
      "source": [
        "# Add project to Python path\n",
        "import sys\n",
        "sys.path.insert(0, '/content/hamletmachine')\n",
        "\n",
        "print(\"‚úÖ Project added to Python path!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload-data-header"
      },
      "source": [
        "## 3. Upload or Process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload-data"
      },
      "outputs": [],
      "source": [
        "# Dataset files are automatically available from GitHub!\n",
        "# The train.jsonl, validation.jsonl, and test.jsonl files are included in the repository.\n",
        "\n",
        "# Verify data files exist (they should be there after cloning from GitHub)\n",
        "data_dir = '/content/hamletmachine/data/processed'\n",
        "required_files = ['train.jsonl', 'validation.jsonl', 'test.jsonl']\n",
        "\n",
        "if os.path.exists(data_dir):\n",
        "    files = os.listdir(data_dir)\n",
        "    jsonl_files = [f for f in files if f.endswith('.jsonl')]\n",
        "    \n",
        "    # Check for required files\n",
        "    missing_files = [f for f in required_files if f not in files]\n",
        "    \n",
        "    if not missing_files:\n",
        "        print(f\"‚úÖ All dataset files found: {required_files}\")\n",
        "        # Show file sizes\n",
        "        for f in required_files:\n",
        "            file_path = os.path.join(data_dir, f)\n",
        "            if os.path.exists(file_path):\n",
        "                size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "                print(f\"   {f}: {size_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Missing files: {missing_files}\")\n",
        "        print(\"   These should be in the repository. If missing, you can:\")\n",
        "        print(\"   1. Re-clone the repository\")\n",
        "        print(\"   2. Or process data on Colab (see Option 2 below)\")\n",
        "    \n",
        "    if jsonl_files:\n",
        "        print(f\"\\nüìÅ All JSONL files in directory: {jsonl_files}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Data directory not found: {data_dir}\")\n",
        "    print(\"   This should exist after cloning from GitHub.\")\n",
        "\n",
        "# Option 2: Process data on Colab (if you need to regenerate)\n",
        "# Uncomment below if you want to process raw training materials:\n",
        "# from hamletmachine.data.pipeline import DataPipeline\n",
        "# pipeline = DataPipeline(config_path='configs/data_config.yaml')\n",
        "# pipeline.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-header"
      },
      "source": [
        "## 4. Configure Training (Auto-Optimized for Your GPU)\n",
        "\n",
        "To train a **bigger model** (e.g. GPT-2 large 774M or XL 1.5B), set `USE_BIGGER_MODEL = True` and `BIGGER_MODEL_NAME = 'gpt2-large'` (or `'gpt2-xl'`) in the next cell. Batch sizes are reduced automatically to avoid OOM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-config"
      },
      "outputs": [],
      "source": [
        "# Auto-configure training based on GPU type\n",
        "import yaml\n",
        "\n",
        "# ========== OPTIONAL: Use a bigger model (override GPU auto-selection) ==========\n",
        "# Set to True to train a larger model; batch sizes will be reduced to avoid OOM.\n",
        "USE_BIGGER_MODEL = False  # Set True to use BIGGER_MODEL_NAME below\n",
        "BIGGER_MODEL_NAME = 'gpt2-large'  # Options: 'gpt2-medium' (350M), 'gpt2-large' (774M), 'gpt2-xl' (1.5B)\n",
        "# GPU guidance: T4 ‚Üí gpt2-medium max; P100/V100 ‚Üí gpt2-large; A100 ‚Üí gpt2-large or gpt2-xl\n",
        "# ================================================================================\n",
        "\n",
        "# GPU-specific optimizations\n",
        "def get_gpu_optimized_config(gpu_type, gpu_memory_gb):\n",
        "    \"\"\"Get optimized config based on GPU type.\"\"\"\n",
        "    \n",
        "    if gpu_type == 'T4':\n",
        "        # T4: Good for small/medium models\n",
        "        return {\n",
        "            'model_architecture': 'gpt2',  # 124M parameters\n",
        "            'batch_size': 8,  # Can go up to 8 on T4 with FP16\n",
        "            'gradient_accumulation': 4,  # Effective batch size: 32\n",
        "            'fp16': True,\n",
        "            'max_seq_length': 1024,\n",
        "        }\n",
        "    elif gpu_type == 'P100':\n",
        "        # P100: Excellent for medium/large models\n",
        "        return {\n",
        "            'model_architecture': 'gpt2-medium',  # 350M parameters\n",
        "            'batch_size': 12,  # P100 can handle larger batches\n",
        "            'gradient_accumulation': 4,  # Effective batch size: 48\n",
        "            'fp16': True,\n",
        "            'max_seq_length': 1024,\n",
        "        }\n",
        "    elif gpu_type == 'V100':\n",
        "        # V100: Excellent for large models\n",
        "        return {\n",
        "            'model_architecture': 'gpt2-medium',  # Can try gpt2-large (774M)\n",
        "            'batch_size': 16,  # V100 can handle even larger batches\n",
        "            'gradient_accumulation': 4,  # Effective batch size: 64\n",
        "            'fp16': True,\n",
        "            'max_seq_length': 1024,\n",
        "        }\n",
        "    elif gpu_type == 'A100':\n",
        "        # A100: Premium, can handle very large models\n",
        "        return {\n",
        "            'model_architecture': 'gpt2-large',  # 774M parameters\n",
        "            'batch_size': 24,  # A100 can handle very large batches\n",
        "            'gradient_accumulation': 4,  # Effective batch size: 96\n",
        "            'fp16': True,\n",
        "            'max_seq_length': 1024,\n",
        "        }\n",
        "    else:\n",
        "        # Conservative defaults for unknown GPUs\n",
        "        return {\n",
        "            'model_architecture': 'gpt2',  # Start small\n",
        "            'batch_size': 4,\n",
        "            'gradient_accumulation': 4,\n",
        "            'fp16': True,\n",
        "            'max_seq_length': 1024,\n",
        "        }\n",
        "\n",
        "# Get optimized config\n",
        "if 'GPU_TYPE' in globals() and GPU_TYPE:\n",
        "    gpu_config = get_gpu_optimized_config(GPU_TYPE, GPU_MEMORY_GB)\n",
        "    print(f\"üéØ Auto-configured for {GPU_TYPE} GPU:\")\n",
        "    print(f\"   Model: {gpu_config['model_architecture']}\")\n",
        "    print(f\"   Batch size: {gpu_config['batch_size']} per device\")\n",
        "    print(f\"   Gradient accumulation: {gpu_config['gradient_accumulation']}\")\n",
        "    print(f\"   Effective batch size: {gpu_config['batch_size'] * gpu_config['gradient_accumulation']}\")\n",
        "    print(f\"   FP16: {gpu_config['fp16']}\")\n",
        "else:\n",
        "    # Fallback if GPU not detected\n",
        "    gpu_config = get_gpu_optimized_config('T4', 16)\n",
        "    print(\"‚ö†Ô∏è  Using default T4 settings (GPU not detected)\")\n",
        "\n",
        "# Apply bigger-model override (reduces batch size to avoid OOM)\n",
        "if USE_BIGGER_MODEL and BIGGER_MODEL_NAME:\n",
        "    bigger = BIGGER_MODEL_NAME\n",
        "    gpu_config['model_architecture'] = bigger\n",
        "    # Memory-safe batch sizes for larger models (reduce if you hit OOM)\n",
        "    if bigger == 'gpt2-xl':  # 1.5B\n",
        "        gpu_config['batch_size'] = min(gpu_config['batch_size'], 2)\n",
        "        gpu_config['gradient_accumulation'] = max(gpu_config['gradient_accumulation'], 8)\n",
        "    elif bigger == 'gpt2-large':  # 774M\n",
        "        gpu_config['batch_size'] = min(gpu_config['batch_size'], 4)\n",
        "        gpu_config['gradient_accumulation'] = max(gpu_config['gradient_accumulation'], 6)\n",
        "    elif bigger == 'gpt2-medium':  # 350M\n",
        "        gpu_config['batch_size'] = min(gpu_config['batch_size'], 8)\n",
        "    print(f\"üìå Override: using bigger model {bigger} (batch_size={gpu_config['batch_size']}, grad_accum={gpu_config['gradient_accumulation']})\")\n",
        "\n",
        "# Create full training config\n",
        "config_path = '/content/hamletmachine/configs/train_config.yaml'\n",
        "if not os.path.exists(config_path):\n",
        "    config = {\n",
        "        'model': {\n",
        "            'architecture': gpu_config['model_architecture'],\n",
        "        },\n",
        "        'training': {\n",
        "            'output_dir': CHECKPOINT_DIR if 'CHECKPOINT_DIR' in globals() else '/content/models/checkpoints',\n",
        "            'num_train_epochs': 3,\n",
        "            'per_device_train_batch_size': gpu_config['batch_size'],\n",
        "            'per_device_eval_batch_size': gpu_config['batch_size'],\n",
        "            'gradient_accumulation_steps': gpu_config['gradient_accumulation'],\n",
        "            'learning_rate': 5.0e-5,\n",
        "            'warmup_steps': 100,\n",
        "            'logging_steps': 10,\n",
        "            'save_steps': 500,\n",
        "            'eval_steps': 500,\n",
        "            'save_total_limit': 3,\n",
        "            'fp16': gpu_config['fp16'],\n",
        "            'dataloader_pin_memory': True,\n",
        "        },\n",
        "        'data': {\n",
        "            'train_file': '/content/hamletmachine/data/processed/train.jsonl',\n",
        "            'validation_file': '/content/hamletmachine/data/processed/validation.jsonl',\n",
        "            'max_seq_length': gpu_config['max_seq_length'],\n",
        "        },\n",
        "        'tokenizer': {\n",
        "            'tokenizer_name': 'gpt2',\n",
        "        },\n",
        "        'logging': {\n",
        "            'logger': 'tensorboard',\n",
        "            'logging_dir': '/content/logs',\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Save config\n",
        "    os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(config, f, default_flow_style=False)\n",
        "    print(f\"\\n‚úÖ Created optimized config at {config_path}\")\n",
        "else:\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    print(f\"\\n‚úÖ Loaded existing config from {config_path}\")\n",
        "    # Override with GPU-optimized settings if not already set\n",
        "    if 'GPU_TYPE' in globals() and GPU_TYPE:\n",
        "        config['model']['architecture'] = gpu_config['model_architecture']\n",
        "        config['training']['per_device_train_batch_size'] = gpu_config['batch_size']\n",
        "        config['training']['per_device_eval_batch_size'] = gpu_config['batch_size']\n",
        "        config['training']['gradient_accumulation_steps'] = gpu_config['gradient_accumulation']\n",
        "        config['training']['fp16'] = gpu_config['fp16']\n",
        "        print(\"   (Updated with GPU-optimized settings)\")\n",
        "\n",
        "print(\"\\nüìã Training Configuration:\")\n",
        "print(yaml.dump(config, default_flow_style=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train-header"
      },
      "source": [
        "## 5. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-model"
      },
      "outputs": [],
      "source": [
        "# Import training modules\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "print(\"‚úÖ Training modules imported!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-tokenizer"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer_name = config['tokenizer']['tokenizer_name']\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"‚úÖ Tokenizer loaded: {tokenizer_name}\")\n",
        "print(f\"Vocabulary size: {len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-model"
      },
      "outputs": [],
      "source": [
        "# Load model (let TrainingArguments handle mixed precision)\n",
        "model_name = config['model']['architecture']\n",
        "use_bf16 = config['training'].get('bf16', False)\n",
        "\n",
        "print(f\"üì• Loading model: {model_name}\")\n",
        "print(f\"   Mixed precision: BF16={use_bf16}\")\n",
        "\n",
        "# Load model in float32 - TrainingArguments will handle mixed precision conversion\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float32  # Let trainer handle mixed precision\n",
        ")\n",
        "\n",
        "# Move to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Calculate model size\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {model_name}\")\n",
        "print(f\"   Parameters: {num_params / 1e6:.2f}M\")\n",
        "print(f\"   Trainable: {num_trainable / 1e6:.2f}M\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "# Check GPU memory usage\n",
        "if torch.cuda.is_available():\n",
        "    memory_allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "    memory_reserved = torch.cuda.memory_reserved(0) / 1e9\n",
        "    print(f\"   GPU Memory - Allocated: {memory_allocated:.2f} GB, Reserved: {memory_reserved:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-dataset"
      },
      "outputs": [],
      "source": [
        "# Load and tokenize datasets\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        max_length=config['data']['max_seq_length'],\n",
        "        padding='max_length'\n",
        "    )\n",
        "\n",
        "print(\"üì• Loading datasets...\")\n",
        "\n",
        "# Load JSONL files\n",
        "train_dataset = load_dataset('json', data_files=config['data']['train_file'], split='train')\n",
        "val_dataset = load_dataset('json', data_files=config['data']['validation_file'], split='train')\n",
        "\n",
        "print(f\"   Training examples: {len(train_dataset)}\")\n",
        "print(f\"   Validation examples: {len(val_dataset)}\")\n",
        "\n",
        "# Tokenize\n",
        "print(\"üî§ Tokenizing datasets...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Tokenizing training set\"\n",
        ")\n",
        "val_dataset = val_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    desc=\"Tokenizing validation set\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Datasets loaded and tokenized!\")\n",
        "print(f\"   Training examples: {len(train_dataset)}\")\n",
        "print(f\"   Validation examples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-trainer"
      },
      "outputs": [],
      "source": [
        "# Setup data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Causal LM, not masked LM\n",
        ")\n",
        "\n",
        "# Setup training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=config['training']['output_dir'],\n",
        "    num_train_epochs=config['training']['num_train_epochs'],\n",
        "    per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n",
        "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
        "    learning_rate=config['training']['learning_rate'],\n",
        "    warmup_steps=config['training']['warmup_steps'],\n",
        "    logging_steps=config['training']['logging_steps'],\n",
        "    save_steps=config['training']['save_steps'],\n",
        "    eval_steps=config['training']['eval_steps'],\n",
        "    save_total_limit=config['training']['save_total_limit'],\n",
        "    fp16=config['training'].get('fp16', False),\n",
        "    bf16=config['training'].get('bf16', False),  # BF16 is more stable than FP16 for T4/P100/V100\n",
        "    dataloader_pin_memory=config['training'].get('dataloader_pin_memory', True),\n",
        "    logging_dir=config['logging']['logging_dir'],\n",
        "    eval_strategy='steps',\n",
        "    save_strategy='steps',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    greater_is_better=False,\n",
        "    report_to='tensorboard' if config['logging']['logger'] == 'tensorboard' else None,\n",
        "    # Gradient clipping for stability\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer setup complete!\")\n",
        "print(f\"   Effective batch size: {config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']}\")\n",
        "print(f\"   Total training steps: {len(train_dataset) // (config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']) * config['training']['num_train_epochs']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start-training"
      },
      "outputs": [],
      "source": [
        "# Start training!\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(f\"   Model: {config['model']['architecture']}\")\n",
        "print(f\"   GPU: {GPU_TYPE if 'GPU_TYPE' in globals() else 'Unknown'}\")\n",
        "print(f\"   Checkpoints: {config['training']['output_dir']}\")\n",
        "print(f\"   TensorBoard: {config['logging']['logging_dir']}\")\n",
        "print(f\"   Epochs: {config['training']['num_train_epochs']}\")\n",
        "print(\"\\nüí° With Colab Pro, you have up to 24 hours - training should complete comfortably!\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\\n‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save-model"
      },
      "outputs": [],
      "source": [
        "# Save final model\n",
        "final_model_dir = os.path.join(config['training']['output_dir'], 'final_model')\n",
        "trainer.save_model(final_model_dir)\n",
        "tokenizer.save_pretrained(final_model_dir)\n",
        "\n",
        "print(f\"‚úÖ Final model saved to: {final_model_dir}\")\n",
        "print(f\"üí° Model is saved to Google Drive and will persist after session ends!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tensorboard-header"
      },
      "source": [
        "## 6. Monitor Training (TensorBoard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tensorboard"
      },
      "outputs": [],
      "source": [
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard\n",
        "%tensorboard --logdir {config['logging']['logging_dir']} --port 6006"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-header"
      },
      "source": [
        "## 7. Download Checkpoints (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-checkpoints"
      },
      "outputs": [],
      "source": [
        "# If checkpoints are saved to Colab storage (not Drive), download them\n",
        "# This creates a zip file you can download\n",
        "\n",
        "import shutil\n",
        "\n",
        "checkpoint_dir = config['training']['output_dir']\n",
        "if os.path.exists(checkpoint_dir) and not checkpoint_dir.startswith('/content/drive'):\n",
        "    zip_path = '/content/hamletmachine_checkpoints.zip'\n",
        "    shutil.make_archive(\n",
        "        zip_path.replace('.zip', ''),\n",
        "        'zip',\n",
        "        checkpoint_dir\n",
        "    )\n",
        "    print(f\"‚úÖ Checkpoints zipped: {zip_path}\")\n",
        "    print(\"Download from: Files ‚Üí hamletmachine_checkpoints.zip\")\n",
        "else:\n",
        "    print(\"‚úÖ Checkpoints are saved to Google Drive - no download needed!\")\n",
        "    print(f\"   Access them at: {checkpoint_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation-header"
      },
      "source": [
        "## 8. Evaluate and Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-trained-model"
      },
      "outputs": [],
      "source": [
        "# Load the trained model for evaluation\n",
        "# Option 1: Use the model already in memory (if you just trained)\n",
        "if 'model' in globals() and 'tokenizer' in globals():\n",
        "    print(\"‚úÖ Using model already loaded in memory\")\n",
        "    eval_model = model\n",
        "    eval_tokenizer = tokenizer\n",
        "else:\n",
        "    # Option 2: Load from saved checkpoint\n",
        "    # The model is saved in the 'final_model' subdirectory\n",
        "    final_model_dir = os.path.join(config['training']['output_dir'], 'final_model')\n",
        "    \n",
        "    # Alternative: Load from a specific checkpoint (e.g., checkpoint-500)\n",
        "    # final_model_dir = os.path.join(config['training']['output_dir'], 'checkpoint-500')\n",
        "    \n",
        "    if not os.path.exists(final_model_dir):\n",
        "        print(f\"‚ö†Ô∏è  Model directory not found: {final_model_dir}\")\n",
        "        print(\"   Available checkpoints:\")\n",
        "        checkpoint_dir = config['training']['output_dir']\n",
        "        if os.path.exists(checkpoint_dir):\n",
        "            for item in os.listdir(checkpoint_dir):\n",
        "                item_path = os.path.join(checkpoint_dir, item)\n",
        "                if os.path.isdir(item_path):\n",
        "                    print(f\"     - {item}\")\n",
        "    else:\n",
        "        print(f\"üì• Loading model from: {final_model_dir}\")\n",
        "        eval_tokenizer = AutoTokenizer.from_pretrained(final_model_dir)\n",
        "        eval_model = AutoModelForCausalLM.from_pretrained(final_model_dir)\n",
        "        \n",
        "        # Move to GPU if available\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        eval_model = eval_model.to(device)\n",
        "        eval_model.eval()  # Set to evaluation mode\n",
        "        \n",
        "        print(f\"‚úÖ Model loaded successfully!\")\n",
        "        print(f\"   Device: {device}\")\n",
        "        print(f\"   Parameters: {sum(p.numel() for p in eval_model.parameters()) / 1e6:.2f}M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quantitative-eval"
      },
      "outputs": [],
      "source": [
        "# Quantitative Evaluation: Calculate metrics on validation and test sets\n",
        "import math\n",
        "\n",
        "print(\"üìä Running quantitative evaluation...\")\n",
        "\n",
        "# Evaluate on validation set (if available)\n",
        "if 'val_dataset' in globals() and val_dataset is not None:\n",
        "    print(\"\\nüîç Evaluating on validation set...\")\n",
        "    val_metrics = trainer.evaluate(eval_dataset=val_dataset)\n",
        "    val_loss = val_metrics.get('eval_loss', 'N/A')\n",
        "    val_perplexity = math.exp(val_loss) if isinstance(val_loss, (int, float)) else 'N/A'\n",
        "    \n",
        "    print(f\"   Validation Loss: {val_loss:.4f}\")\n",
        "    print(f\"   Validation Perplexity: {val_perplexity:.4f if isinstance(val_perplexity, (int, float)) else val_perplexity}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Validation dataset not available in memory\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nüîç Evaluating on test set...\")\n",
        "test_file = config['data'].get('test_file', '/content/hamletmachine/data/processed/test.jsonl')\n",
        "\n",
        "if os.path.exists(test_file):\n",
        "    # Load and tokenize test set\n",
        "    test_ds = load_dataset('json', data_files=test_file, split='train')\n",
        "    \n",
        "    def tokenize_function(examples):\n",
        "        return eval_tokenizer(\n",
        "            examples['text'],\n",
        "            truncation=True,\n",
        "            max_length=config['data']['max_seq_length'],\n",
        "            padding='max_length'\n",
        "        )\n",
        "    \n",
        "    test_dataset = test_ds.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        remove_columns=test_ds.column_names\n",
        "    )\n",
        "    \n",
        "    # Create a temporary trainer for evaluation\n",
        "    from transformers import Trainer, TrainingArguments\n",
        "    \n",
        "    eval_args = TrainingArguments(\n",
        "        output_dir='/tmp/eval',\n",
        "        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n",
        "        fp16=False,\n",
        "        bf16=False,\n",
        "    )\n",
        "    \n",
        "    eval_trainer = Trainer(\n",
        "        model=eval_model,\n",
        "        args=eval_args,\n",
        "        eval_dataset=test_dataset,\n",
        "    )\n",
        "    \n",
        "    test_metrics = eval_trainer.evaluate()\n",
        "    test_loss = test_metrics.get('eval_loss', 'N/A')\n",
        "    test_perplexity = math.exp(test_loss) if isinstance(test_loss, (int, float)) else 'N/A'\n",
        "    \n",
        "    print(f\"   Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"   Test Perplexity: {test_perplexity:.4f if isinstance(test_perplexity, (int, float)) else test_perplexity}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Test file not found: {test_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qualitative-test"
      },
      "outputs": [],
      "source": [
        "# Qualitative Testing: Generate text samples\n",
        "import torch\n",
        "\n",
        "def generate_text(prompt, max_new_tokens=150, temperature=0.8, top_p=0.95, top_k=50):\n",
        "    \"\"\"Generate text from a prompt.\"\"\"\n",
        "    # Tokenize input\n",
        "    inputs = eval_tokenizer(prompt, return_tensors=\"pt\").to(eval_model.device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        output_ids = eval_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            pad_token_id=eval_tokenizer.eos_token_id,\n",
        "            eos_token_id=eval_tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    # Decode output\n",
        "    generated_text = eval_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Return only the newly generated part (remove the prompt)\n",
        "    if generated_text.startswith(prompt):\n",
        "        return generated_text[len(prompt):].strip()\n",
        "    return generated_text.strip()\n",
        "\n",
        "print(\"üé≠ Testing text generation...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test prompts related to your training data\n",
        "test_prompts = [\n",
        "    \"HAMLETMASCHINE:\\n\\n\",\n",
        "    \"Heiner M√ºller writes about Hamletmachine as\",\n",
        "    \"The critique of political economy begins with\",\n",
        "    \"In the beginning was the word, and the word was\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\nüìù Prompt {i}: {repr(prompt)}\")\n",
        "    print(\"-\" * 60)\n",
        "    generated = generate_text(prompt, max_new_tokens=200, temperature=0.7)\n",
        "    print(generated)\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n‚úÖ Text generation testing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive-generation"
      },
      "outputs": [],
      "source": [
        "# Interactive text generation - customize your prompts here\n",
        "\n",
        "# Try your own prompts!\n",
        "custom_prompt = \"HAMLETMASCHINE:\\n\\n\"  # Change this to test different prompts\n",
        "\n",
        "print(f\"üìù Generating text from prompt: {repr(custom_prompt)}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "generated = generate_text(\n",
        "    custom_prompt,\n",
        "    max_new_tokens=300,  # Adjust length\n",
        "    temperature=0.8,      # Lower = more focused, Higher = more creative\n",
        "    top_p=0.95,          # Nucleus sampling\n",
        "    top_k=50            # Top-k sampling\n",
        ")\n",
        "\n",
        "print(generated)\n",
        "print(\"-\" * 60)\n",
        "print(\"\\nüí° Tips:\")\n",
        "print(\"   - Lower temperature (0.5-0.7) = more conservative, focused text\")\n",
        "print(\"   - Higher temperature (0.8-1.2) = more creative, diverse text\")\n",
        "print(\"   - Adjust max_new_tokens to control output length\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
