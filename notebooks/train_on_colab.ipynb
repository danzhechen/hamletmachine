{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Hamletmachine LLM Training on Google Colab\n",
    "\n",
    "This notebook trains the hamletmachine language model using Google Colab's free GPU.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU (T4)\n",
    "2. **Mount Google Drive** (optional, for saving checkpoints): Run the mount cell below\n",
    "3. **Upload your project**: Either clone from GitHub or upload the project folder\n",
    "4. **Run all cells** in order\n",
    "\n",
    "## Notes\n",
    "- Free tier: ~9-12 hour sessions, may disconnect\n",
    "- Checkpoints are saved to Google Drive (if mounted) or Colab storage\n",
    "- Training progress is logged to TensorBoard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected! Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional - for saving checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set checkpoint directory (change path as needed)\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/hamletmachine/checkpoints'\n",
    "import os\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Option 1: Clone from GitHub (if your repo is on GitHub)\n",
    "# !git clone https://github.com/yourusername/hamletmachine.git\n",
    "# %cd hamletmachine\n",
    "\n",
    "# Option 2: Upload project folder manually\n",
    "# 1. Click folder icon on left sidebar\n",
    "# 2. Upload your project folder\n",
    "# 3. Uncomment and adjust path below:\n",
    "\n",
    "# %cd /content/hamletmachine  # Adjust path if needed\n",
    "\n",
    "# For now, we'll work in /content directory\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-header"
   },
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install project dependencies\n",
    "!pip install -q transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0 tokenizers>=0.15.0\n",
    "!pip install -q torch>=2.1.0 numpy>=1.24.0 pandas>=2.0.0 pyyaml>=6.0\n",
    "!pip install -q tensorboard>=2.15.0 tqdm>=4.66.0\n",
    "\n",
    "# Optional: Install WandB for experiment tracking\n",
    "# !pip install -q wandb\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-project"
   },
   "outputs": [],
   "source": [
    "# Install the project package (if using GitHub clone)\n",
    "# !pip install -e .\n",
    "\n",
    "# Or add to Python path if uploaded manually\n",
    "import sys\n",
    "sys.path.insert(0, '/content/hamletmachine')\n",
    "\n",
    "print(\"‚úÖ Project added to Python path!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload-data-header"
   },
   "source": [
    "## 3. Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-data"
   },
   "outputs": [],
   "source": [
    "# Option 1: Upload processed dataset files\n",
    "# Use the file browser on the left to upload:\n",
    "# - data/processed/train.jsonl\n",
    "# - data/processed/validation.jsonl\n",
    "# - data/processed/test.jsonl\n",
    "\n",
    "# Option 2: Process data on Colab (if you uploaded raw training materials)\n",
    "# from hamletmachine.data.pipeline import DataPipeline\n",
    "# pipeline = DataPipeline(config_path='configs/data_config.yaml')\n",
    "# pipeline.run()\n",
    "\n",
    "# Verify data files exist\n",
    "import os\n",
    "data_dir = '/content/hamletmachine/data/processed'\n",
    "if os.path.exists(data_dir):\n",
    "    files = os.listdir(data_dir)\n",
    "    print(f\"Data files found: {files}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Data directory not found: {data_dir}\")\n",
    "    print(\"Please upload your processed dataset files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config-header"
   },
   "source": [
    "## 4. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-config"
   },
   "outputs": [],
   "source": [
    "# Load or create training configuration\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to load existing config\n",
    "config_path = '/content/hamletmachine/configs/train_config.yaml'\n",
    "if not os.path.exists(config_path):\n",
    "    # Create default config for Colab\n",
    "    config = {\n",
    "        'model': {\n",
    "            'architecture': 'gpt2',  # Start with small model\n",
    "        },\n",
    "        'training': {\n",
    "            'output_dir': CHECKPOINT_DIR if 'CHECKPOINT_DIR' in globals() else '/content/models/checkpoints',\n",
    "            'num_train_epochs': 3,\n",
    "            'per_device_train_batch_size': 4,  # Adjust based on GPU memory\n",
    "            'per_device_eval_batch_size': 4,\n",
    "            'gradient_accumulation_steps': 4,\n",
    "            'learning_rate': 5.0e-5,\n",
    "            'warmup_steps': 100,\n",
    "            'logging_steps': 10,\n",
    "            'save_steps': 500,\n",
    "            'eval_steps': 500,\n",
    "            'save_total_limit': 3,\n",
    "            'fp16': True,  # Enable for T4 GPU\n",
    "        },\n",
    "        'data': {\n",
    "            'train_file': '/content/hamletmachine/data/processed/train.jsonl',\n",
    "            'validation_file': '/content/hamletmachine/data/processed/validation.jsonl',\n",
    "            'max_seq_length': 1024,\n",
    "        },\n",
    "        'tokenizer': {\n",
    "            'tokenizer_name': 'gpt2',\n",
    "        },\n",
    "        'logging': {\n",
    "            'logger': 'tensorboard',\n",
    "            'logging_dir': '/content/logs',\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save config\n",
    "    os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "    print(f\"‚úÖ Created default config at {config_path}\")\n",
    "else:\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(f\"‚úÖ Loaded config from {config_path}\")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train-header"
   },
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-model"
   },
   "outputs": [],
   "source": [
    "# Import training modules\n",
    "# Note: This will work once the training module is implemented\n",
    "# For now, this is a template structure\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "print(\"‚úÖ Training modules imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-tokenizer"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer_name = config['tokenizer']['tokenizer_name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded: {tokenizer_name}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_name = config['model']['architecture']\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if config['training']['fp16'] else torch.float32\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model_name}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-dataset"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=config['data']['max_seq_length'],\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "# Load JSONL files\n",
    "train_dataset = load_dataset('json', data_files=config['data']['train_file'], split='train')\n",
    "val_dataset = load_dataset('json', data_files=config['data']['validation_file'], split='train')\n",
    "\n",
    "# Tokenize\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Datasets loaded!\")\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-trainer"
   },
   "outputs": [],
   "source": [
    "# Setup data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config['training']['output_dir'],\n",
    "    num_train_epochs=config['training']['num_train_epochs'],\n",
    "    per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "    learning_rate=config['training']['learning_rate'],\n",
    "    warmup_steps=config['training']['warmup_steps'],\n",
    "    logging_steps=config['training']['logging_steps'],\n",
    "    save_steps=config['training']['save_steps'],\n",
    "    eval_steps=config['training']['eval_steps'],\n",
    "    save_total_limit=config['training']['save_total_limit'],\n",
    "    fp16=config['training']['fp16'],\n",
    "    logging_dir=config['logging']['logging_dir'],\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='tensorboard' if config['logging']['logger'] == 'tensorboard' else None,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start-training"
   },
   "outputs": [],
   "source": [
    "# Start training!\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"Checkpoints will be saved to: {config['training']['output_dir']}\")\n",
    "print(f\"TensorBoard logs: {config['logging']['logging_dir']}\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-model"
   },
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_dir = os.path.join(config['training']['output_dir'], 'final_model')\n",
    "trainer.save_model(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "print(f\"‚úÖ Final model saved to: {final_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tensorboard-header"
   },
   "source": [
    "## 6. Monitor Training (TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tensorboard"
   },
   "outputs": [],
   "source": [
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard\n",
    "%tensorboard --logdir {config['logging']['logging_dir']} --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-header"
   },
   "source": [
    "## 7. Download Checkpoints (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-checkpoints"
   },
   "outputs": [],
   "source": [
    "# If checkpoints are saved to Colab storage (not Drive), download them\n",
    "# This creates a zip file you can download\n",
    "\n",
    "import shutil\n",
    "\n",
    "checkpoint_dir = config['training']['output_dir']\n",
    "if os.path.exists(checkpoint_dir) and not checkpoint_dir.startswith('/content/drive'):\n",
    "    zip_path = '/content/hamletmachine_checkpoints.zip'\n",
    "    shutil.make_archive(\n",
    "        zip_path.replace('.zip', ''),\n",
    "        'zip',\n",
    "        checkpoint_dir\n",
    "    )\n",
    "    print(f\"‚úÖ Checkpoints zipped: {zip_path}\")\n",
    "    print(\"Download from: Files ‚Üí hamletmachine_checkpoints.zip\")\n",
    "else:\n",
    "    print(\"‚úÖ Checkpoints are saved to Google Drive - no download needed!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
